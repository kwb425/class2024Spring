{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwb425/class2024Spring/blob/main/class2024Spring_0517-0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTLBHelPpbWU"
      },
      "source": [
        "# langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrzPy9_j74f4"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchain-core\n",
        "!pip install -q langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUHU4juZnqat"
      },
      "source": [
        "# ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0AyNOyH4BIZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2xkX4F9_C7T"
      },
      "source": [
        "Once the xterm terminal is opened below, type the two commands to install Ollama: \\\n",
        "curl -fsSL https://ollama.com/install.sh | sh \\\n",
        "ollama serve & ollama run llama3 (Error: listen tcp 127.0.0.1:11434: bind: address already in use 무시하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-RmgYWvBlNH"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv73ym0qpU6V"
      },
      "outputs": [],
      "source": [
        "# Ollama 임베딩, 모델 로드 및 테스트\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "model = ChatOllama(model=\"llama3\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9UFlqOVBckb"
      },
      "outputs": [],
      "source": [
        "model.invoke(\"겨울철에 내한성이 강한 나무에는 어떤 것이 있을까요?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbE2ufbon7Ov"
      },
      "source": [
        "# chatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFUAUQcEn62Q"
      },
      "outputs": [],
      "source": [
        "# chatGPT 모델 로드 및 테스트\n",
        "!pip install -q langchain-openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
        "\n",
        "model = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdWJyfBj_2Kz"
      },
      "outputs": [],
      "source": [
        "model.invoke(\"겨울철에 내한성이 강한 나무에는 어떤 것이 있을까요?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXpmD4rmv7dn"
      },
      "source": [
        "# huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgwT7sSCv6o_"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n",
        "!pip install -q torch transformers accelerate bitsandbytes\n",
        "import huggingface_hub\n",
        "\n",
        "huggingface_hub.login('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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Qk89P3pfaD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FopUlmkAqkyb"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "model = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yelo5AHCm-66"
      },
      "outputs": [],
      "source": [
        "model.invoke(\"겨울철에 내한성이 강한 나무에는 어떤 것이 있을까요?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zlXfbrI_abi"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZMCBujg-TxP"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence_transformers\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
        "\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name='jhgan/ko-sroberta-nli',\n",
        "#     model_kwargs={'device':'cpu'},\n",
        "#     encode_kwargs={'normalize_embeddings':True},\n",
        "# )\n",
        "\n",
        "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUsQTZlyn1Ed"
      },
      "source": [
        "# prepare RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EggYZFaiD25y"
      },
      "outputs": [],
      "source": [
        "# PDF 문서 로드 및 텍스트 추출\n",
        "!pip install pypdf\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/class2024Spring_0517-1.pdf\", extract_images=False)\n",
        "pages = loader.load()\n",
        "pages[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLGhYbJqEyLi"
      },
      "outputs": [],
      "source": [
        "# 문장 임베딩 및 벡터 저장소 생성\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 문서를 문장으로 분리\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "splits = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVA3A28bQIV"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(splits, embeddings)\n",
        "\n",
        "# from langchain.vectorstores import FAISS\n",
        "# vectorstore = FAISS.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh52jht0GNBm"
      },
      "outputs": [],
      "source": [
        "# 검색 쿼리\n",
        "query = \"겨울철에 내한성이 강한 나무에는 어떤 것이 있을까요?\"\n",
        "\n",
        "# 가장 유사도가 높은 5 문장을 하나만 추출\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
        "docs = retriever.get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAzuHS4Qq-tN"
      },
      "source": [
        "# execute chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UU_Rsn_GYoV"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Prompt\n",
        "template = '''Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "'''\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return '\\n\\n'.join([d.page_content for d in docs])\n",
        "\n",
        "# RAG Chain 연결\n",
        "rag_chain = (\n",
        "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLs6RwQxKMTn"
      },
      "outputs": [],
      "source": [
        "query = \"겨울철에 내한성이 강한 나무에는 어떤 것이 있을까요?\"\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ1MkmCzGvST"
      },
      "outputs": [],
      "source": [
        "query = \"겨울철에 추위에 약한 나무에는 어떤 것이 있을까요?\"\n",
        "rag_chain.invoke(query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
